{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy and Pandas\n",
    "\n",
    "## Objectives: \n",
    "\n",
    "- Use numpy.random to generate a dataset.\n",
    "- Read in various forms of data into a panadas dataframe\n",
    "- Perform some basic operations on the dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "- Review Numpy\n",
    "- Introduce Pandas\n",
    "- Reading in data and merging dataframes (DF)\n",
    "- working with missing values\n",
    "- Slicing, selecing and xxtracting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is nothing in NumPy which can’t be done via python lists or by using other data structures.\n",
    "- NumPy provides an efficient storage and better way handling of the data for mathematical operations using simple API’s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of 25 numbers between -1 and 1\n",
    "pure = np.linspace(-1, 1, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatterplot where the x and y both refer to the array created above\n",
    "plt.plot(pure, pure, 'o', color='black');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add some noise to the data by randomly generating numbers between 0 and .5 to add to the Ys\n",
    "noise = np.random.normal(0, .5, pure.shape)\n",
    "signal = pure + noise\n",
    "plt.plot(pure, signal, 'o', color='black');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph a log transformation of the numbers\n",
    "pure = np.linspace(0, 10000, 50)\n",
    "noise = np.random.normal(-.2, .2, pure.shape)\n",
    "y = np.log(pure)\n",
    "signal = y + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(pure, signal)\n",
    "ax.set_title('Log Transformation scatter plot')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use a numpy function to transform the `pure` data series and then create your own scatterplot of the new data with the `pure` data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#transform your data with a numpy function\n",
    "____ = np._____\n",
    "\n",
    "#plot the newly retransformed graph\n",
    "ax.scatter(pure, ____)\n",
    "\n",
    "#rename your grpah\n",
    "ax.set_title('______')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/flatironschool/PROJECTS/FabJayProject/FabJayProject'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JSON_and_API_calls-JayCopy1.ipynb',\n",
       " 'IMDB-Movie-Data.csv',\n",
       " '.DS_Store',\n",
       " 'Intro_to_Pandas-JayCopy1.ipynb',\n",
       " 'README.md',\n",
       " '.gitignore',\n",
       " 'Data_cleaning-JayCopy1.ipynb',\n",
       " 'macrodata.csv',\n",
       " 'series-and-dataframe.png',\n",
       " '.ipynb_checkpoints',\n",
       " 'foods-2011-10-03.json',\n",
       " 'joinimages.png',\n",
       " '.git',\n",
       " 'movie_metadata.csv',\n",
       " 'nyc_edu_files',\n",
       " '1617FedSchoolCodeList.xlsx']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/flatironschool/PROJECTS/FabJayProject/FabJayProject/nyc_edu_files/TestResults'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core components of pandas: Series and DataFrames\n",
    "\n",
    "The primary two components of pandas are the `Series` and `DataFrame`.\n",
    "\n",
    "A Series is essentially a column, and a DataFrame is a multi-dimensional table made up of a collection of Series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"series-and-dataframe.png\" alt=\"drawing\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Dataframe\n",
    "\n",
    "A dataframe can be created from many different types of data sources.  Below are diffeten examples of this.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create from dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a ditionary containing employee data\n",
    "data1 = {'Name':['Jai', 'Princi', 'Gaurav', 'Anuj'], \n",
    "        'Age':[27, 24, 22, 32], \n",
    "        'Address':['Nagpur', 'Kanpur', 'Allahabad', 'Kannuaj'], \n",
    "        'Qualification':['Msc', 'MA', 'MCA', 'Phd'],\n",
    "        'Mobile No': [97, 91, 58, 76]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the dictionary into DataFrame \n",
    "df1 = pd.DataFrame(data1,index=[0, 1, 2, 3])\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataframe from a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../nyc edu files/\\xe2\\x81\\xa8Test Results/2012_SAT_Results.csv' does not exist: b'../nyc edu files/\\xe2\\x81\\xa8Test Results/2012_SAT_Results.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c57a8e832c2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../nyc edu files/⁨Test Results/2012_SAT_Results.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mjdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../nyc edu files/\\xe2\\x81\\xa8Test Results/2012_SAT_Results.csv' does not exist: b'../nyc edu files/\\xe2\\x81\\xa8Test Results/2012_SAT_Results.csv'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "478"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jdata = pd.read_csv(\"2012_SAT_Results.csv\")\n",
    "print(len(jdata[jdata['Num of SAT Test Takers'] == 's']))\n",
    "len(jdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/flatironschool/PROJECTS/FabJayProject/FabJayProject/nyc_edu_files/EverythingElse\n"
     ]
    }
   ],
   "source": [
    "cd EverythingElse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017-2018_hs_sqr_results.xlsx',\n",
       " 'DetailedAccountabilityStatus.xlsx',\n",
       " '2010-2011_Class_Size_-_School-level_detail.csv',\n",
       " 'schoolqualityreports_comparisongroupdescription_2017.pdf',\n",
       " '2014-15-to-2018-19_demographic-snapshot.xlsx',\n",
       " '2005_-_2018_Quality_Review_Ratings_DD_KEYS.xlsx',\n",
       " '2019-public-data-file_teacher.xlsx',\n",
       " 'School_Survey_Data_Dictionary_KEYS.xls',\n",
       " '2010-2011_Class_Size_Open_Data_Dictionary.xlsx',\n",
       " '2010_-_2011_School_Attendance_and_Enrollment_Statistics_by_District.csv',\n",
       " '2005_-_2018_Quality_Review_Ratings.csv']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179040"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdata = pd.read_csv('2016-2017_Graduation_Outcomes_School.csv')\n",
    "len(kdata[kdata['Total Grads #'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DBN                                  object\n",
       "School Name                          object\n",
       "Demographic Category                 object\n",
       "Demographic Variable                 object\n",
       "Cohort Year                           int64\n",
       "Cohort                               object\n",
       "Total Cohort #                        int64\n",
       "Total Grads #                       float64\n",
       "Total Grads % of cohort             float64\n",
       "Total Regents #                     float64\n",
       "Total Regents % of cohort           float64\n",
       "Total Regents % of grads            float64\n",
       "Advanced Regents #                  float64\n",
       "Advanced Regents % of cohort        float64\n",
       "Advanced Regents % of grads         float64\n",
       "Regents w/o Advanced #              float64\n",
       "Regents w/o Advanced % of cohort    float64\n",
       "Regents w/o Advanced % of grads     float64\n",
       "Local #                             float64\n",
       "Local % of cohort                   float64\n",
       "Local % of grads                    float64\n",
       "Still Enrolled #                    float64\n",
       "Still Enrolled % of cohort          float64\n",
       "Dropped Out #                       float64\n",
       "Dropped Out % of cohort             float64\n",
       "SACC (IEP Diploma) #                float64\n",
       "SACC (IEP Diploma) % of cohort      float64\n",
       "TASC (GED) #                        float64\n",
       "TASC (GED) % of cohort              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kdata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaydata = kdata[kdata['Cohort Year'] == 2013]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(kaydata, sqr_summary, on='DBN', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DBN', 'School Name', 'School Type', 'Enrollment',\n",
       "       'Rigorous Instruction Rating', 'Collaborative Teachers Rating',\n",
       "       'Supportive Environment Rating', 'Effective School Leadership Rating',\n",
       "       'Strong Family-Community Ties Rating', 'Trust Rating',\n",
       "       'Student Achievement Rating', 'Rigorous Instruction - Percent Positive',\n",
       "       'Collaborative Teachers - Percent Positive',\n",
       "       'Supportive Environment - Percent Positive',\n",
       "       'Effective School Leadership - Percent Positive',\n",
       "       'Strong Family-Community Ties - Percent Positive',\n",
       "       'Trust - Percent Positive',\n",
       "       'Quality Review - How interesting and challenging is the curriculum?',\n",
       "       'Quality Review - How effective is the teaching and learning?',\n",
       "       'Quality Review - How well does the school assess what students are learning?',\n",
       "       'Quality Review - How clearly are high expectations communicated to students and staff?',\n",
       "       'Quality Review - How well do teachers work with each other?',\n",
       "       'Quality Review - How safe and inclusive is the school while supporting social-emotional growth?',\n",
       "       'Quality Review - How well does the school allocate and manage resources?',\n",
       "       'Quality Review - How well does the school identify, track, and meet its goals?',\n",
       "       'Quality Review - How thoughtful is the school’s approach to teacher development and evaluation?',\n",
       "       'Quality Review - How well are school decisions evaluated and adjusted?',\n",
       "       'Quality Review - Dates of Review',\n",
       "       'Average Grade 8 English Proficiency',\n",
       "       'Average Grade 8 Math Proficiency', 'Percent English Language Learners',\n",
       "       'Percent Students with Disabilities', 'Percent Self-Contained',\n",
       "       'Economic Need Index', 'Percent Overage/Undercredited',\n",
       "       'Percent in Temp Housing', 'Percent HRA Eligible', 'Percent Asian',\n",
       "       'Percent Black', 'Percent Hispanic', 'Percent White',\n",
       "       'Years of principal experience at this school',\n",
       "       'Percent of teachers with 3 or more years of experience',\n",
       "       'Student Attendance Rate', 'Percent of Students Chronically Absent',\n",
       "       'Teacher Attendance Rate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaydata.columns\n",
    "sqr_summary.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DBN', 'School Name_x', 'Demographic Category', 'Demographic Variable',\n",
       "       'Cohort Year', 'Cohort', 'Total Cohort #', 'Total Grads #',\n",
       "       'Total Grads % of cohort', 'Total Regents #',\n",
       "       'Total Regents % of cohort', 'Total Regents % of grads',\n",
       "       'Advanced Regents #', 'Advanced Regents % of cohort',\n",
       "       'Advanced Regents % of grads', 'Regents w/o Advanced #',\n",
       "       'Regents w/o Advanced % of cohort', 'Regents w/o Advanced % of grads',\n",
       "       'Local #', 'Local % of cohort', 'Local % of grads', 'Still Enrolled #',\n",
       "       'Still Enrolled % of cohort', 'Dropped Out #',\n",
       "       'Dropped Out % of cohort', 'SACC (IEP Diploma) #',\n",
       "       'SACC (IEP Diploma) % of cohort', 'TASC (GED) #',\n",
       "       'TASC (GED) % of cohort', 'School Name_y', 'School Type', 'Enrollment',\n",
       "       'Rigorous Instruction Rating', 'Collaborative Teachers Rating',\n",
       "       'Supportive Environment Rating', 'Effective School Leadership Rating',\n",
       "       'Strong Family-Community Ties Rating', 'Trust Rating',\n",
       "       'Student Achievement Rating', 'Rigorous Instruction - Percent Positive',\n",
       "       'Collaborative Teachers - Percent Positive',\n",
       "       'Supportive Environment - Percent Positive',\n",
       "       'Effective School Leadership - Percent Positive',\n",
       "       'Strong Family-Community Ties - Percent Positive',\n",
       "       'Trust - Percent Positive',\n",
       "       'Quality Review - How interesting and challenging is the curriculum?',\n",
       "       'Quality Review - How effective is the teaching and learning?',\n",
       "       'Quality Review - How well does the school assess what students are learning?',\n",
       "       'Quality Review - How clearly are high expectations communicated to students and staff?',\n",
       "       'Quality Review - How well do teachers work with each other?',\n",
       "       'Quality Review - How safe and inclusive is the school while supporting social-emotional growth?',\n",
       "       'Quality Review - How well does the school allocate and manage resources?',\n",
       "       'Quality Review - How well does the school identify, track, and meet its goals?',\n",
       "       'Quality Review - How thoughtful is the school’s approach to teacher development and evaluation?',\n",
       "       'Quality Review - How well are school decisions evaluated and adjusted?',\n",
       "       'Quality Review - Dates of Review',\n",
       "       'Average Grade 8 English Proficiency',\n",
       "       'Average Grade 8 Math Proficiency', 'Percent English Language Learners',\n",
       "       'Percent Students with Disabilities', 'Percent Self-Contained',\n",
       "       'Economic Need Index', 'Percent Overage/Undercredited',\n",
       "       'Percent in Temp Housing', 'Percent HRA Eligible', 'Percent Asian',\n",
       "       'Percent Black', 'Percent Hispanic', 'Percent White',\n",
       "       'Years of principal experience at this school',\n",
       "       'Percent of teachers with 3 or more years of experience',\n",
       "       'Student Attendance Rate', 'Percent of Students Chronically Absent',\n",
       "       'Teacher Attendance Rate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV from weburl\n",
    "\n",
    "*If you are doing this from git, make sure you have the url for the raw csv file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sat_results = pd.read_csv('2012_SAT_Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, sat_results, on='DBN', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14178, 79)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in excel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a file called `1617FedSchoolCodeList.xlsx` in this directory.Look up how to read an excel file into a dataframe and do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqr_summary = pd.read_excel('2017-2018_hs_sqr_results.xlsx', sheet_name='Summary', skiprows=1)\n",
    "sqr_sa = pd.read_excel('2017-2018_hs_sqr_results.xlsx', sheet_name='Student Achievement', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2006-2012 Math Test Results\u001b[m\u001b[m/\r\n",
      "2010_SAT_Data_Dictionary.xlsx\r\n",
      "2010_SAT__College_Board_School_Level_Results.csv\r\n",
      "2012_SAT_Results.csv\r\n",
      "2012_SAT_Results_Data_Dictionary_.xlsx\r\n",
      "2013-2018_Borough_Math_Results.xlsx\r\n",
      "\u001b[34m2013-2018_English:ELA Test Results\u001b[m\u001b[m/\r\n",
      "\u001b[34m2013-2018_Math Test Results\u001b[m\u001b[m/\r\n",
      "2014-15__2017-18-nyc-regents-results.xlsx\r\n",
      "charter-school-test-results-2013-2018.xlsx\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create from  from json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "db = json.load(open('foods-2011-10-03.json'))\n",
    "len(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_json('foods-2011-10-03.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_keys = ['description', 'group', 'id', 'manufacturer']\n",
    "info = pd.DataFrame(db, columns=info_keys)\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can can use [pd.read_sql()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html) sql queries and use pandas to execute them and put the results in a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas.read_sql(sql, con, index_col=None, coerce_float=True, params=None, parse_dates=None, columns=None, chunksize=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector import pandas as pd\n",
    "\n",
    "cnx = mysql.connector .connect(\n",
    "    host = \"localhost\",\n",
    "    user = \"root\",\n",
    "    passwd = \"dbms\"\n",
    ")\n",
    "\n",
    "\n",
    "SQL_Query = pd.read_sql_query(\n",
    "'''select\n",
    "product_name,\n",
    "product_price_per_unit,\n",
    "units_ordered,\n",
    "((units_ordered) * (product_price_per_unit)) AS revenue\n",
    "from tracking_sales''', conn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(SQL_Query, columns=['product_name','product_price_per_unit','units_ordered','revenue'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familiarizing yourself with the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#look at the columns attribute\n",
    "print(df1.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the shape attribute\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#understanding the different types of data for each column\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call on a specific column\n",
    "\n",
    "df1['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get descriptive stats by columns\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Why did we only get the summary stats for 2 columns?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a second data frame of employee data to merge the two \n",
    "data2 = {'Name':['Gaurav', 'Anuj', 'Dhiraj', 'Hitesh'], \n",
    "        'Age':[22, 32, 12, 52], \n",
    "        'Address':['Allahabad', 'Kannuaj', 'Allahabad', 'Kannuaj'], \n",
    "        'Qualification':['MCA', 'Phd', 'Bcom', 'B.hons'],\n",
    "        'Salary':[1000, 2000, 3000, 4000]} \n",
    "\n",
    "df2 = pd.DataFrame(data2, index=[2, 3, 6, 7]) \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appending Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.append(df2, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], sort=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df1, df2], sort=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"joinimages.png\" alt=\"drawing\" width=\"550\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the merge function on multiple dataframes without any specifications, it tries its best to merge them. It will assess any column that they find to be identical and use those as the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to keep all data we should do an outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are only concerned with data from one table, then you can specify that table is what you want to merge on.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you want to be specific in which columns you want the dataframe to merge on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2, how='right', on=['Name', 'Age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation\n",
    "\n",
    "### Removing duplicates\n",
    "\n",
    "Duplicate rows may be found in a DataFrame for any number of reasons. Here is an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'k1': ['one'] * 3 + ['two'] * 4,\n",
    "                  'k2': [1, 1, 2, 3, 3, 4, 4]})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method that returns a boolean Series indicating whether each row \n",
    "# is a duplicate or not\n",
    "data[data.duplicated()]\n",
    "\n",
    "data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I call `data` again why are there still duplicates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'k1': ['one'] * 3 + ['two'] * 4,\n",
    "                  'k2': [1, 1, 2, 3, 3, 4, 4]})\n",
    "\n",
    "#we are adding another column to help us understand which row was dropped\n",
    "data['v1'] = range(7)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.drop_duplicates(['k1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data.drop_duplicates(['k1'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining different parts of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('IMDB-Movie-Data.csv', index_col='Title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll look at getting data by rows.\n",
    "\n",
    "For rows, we have two options:\n",
    "\n",
    "- .loc - locates by name\n",
    "- .iloc- locates by numerical index\n",
    "\n",
    "Remember that we are still indexed by movie Title, so to use .loc we give it the Title of a movie:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use loc to find the row by name\n",
    "prom = movies_df.loc[\"Prometheus\"]\n",
    "\n",
    "prom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use iloc to find the row by numerical index\n",
    "\n",
    "prom = movies_df.iloc[1]\n",
    "prom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loc` and `iloc` can be thought of as similar to Python list slicing. To show this even further, let's select multiple rows.\n",
    "\n",
    "\n",
    "How would you do it with a list? In Python, just slice with brackets like `example_list[1:4]`. It's works the same way in pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.loc['Prometheus':'Sing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.iloc[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Copy vs. Shallow Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy of the dataframe. \n",
    "testing = movies_df\n",
    "testing.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the values for one column\n",
    "testing['Director'] = 'SeanAbu'\n",
    "testing.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Let's look back at the origianl dataframe***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a shallow copy of the dataframe.  What does that mean?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([1, 2], index=[\"a\", \"b\"])\n",
    "deep = s.copy(deep=True)\n",
    "shallow = s.copy(deep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shallow copy shares data and index with original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s is shallow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.values is shallow.values and s.index is shallow.index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep copy has own copy of data and index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s is deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.values is deep.values or s.index is deep.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updates to the data shared by shallow copy and original is reflected in both; deep copy remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[0] = 3\n",
    "shallow[1] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional selections\n",
    "\n",
    "\n",
    "For example, what if we want to filter our movies DataFrame to show only films directed by Ridley Scott or films with a rating greater than or equal to 8.0?\n",
    "\n",
    "To do that, we take a column from the DataFrame and apply a Boolean condition to it. Here's an example of a Boolean condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('IMDB-Movie-Data.csv', index_col='Title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In SQL\n",
    "\n",
    "`select * from table where column_name = some_value`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "condition = (movies_df['Director'] == \"Ridley Scott\")\n",
    "\n",
    "condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to filter out all movies not directed by Ridley Scott, in other words, we don’t want the False films. To return the rows where that condition is True we have to pass this operation into the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_df[condition_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_df[movies_df['Director'] == \"Ridley Scott\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find how many movies were directed by Christopher Nolan.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at conditional selections using numerical values by filtering the DataFrame by ratings:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[movies_df['Rating'] >= 8.8]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make some richer conditionals by using logical operators: \n",
    "- `|`    for \"or\"  \n",
    "- `&`    for \"and\"\n",
    "\n",
    "\n",
    "Let's filter the the DataFrame to show only movies by Christopher Nolan OR Ridley Scott:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[(movies_df['Director'] == 'Christopher Nolan') | (movies_df['Director'] == 'Ridley Scott')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Find all of the movies by a Christopher Nolan that have a score of 8.7 or better**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `isin()` method we could make this more concise though:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[movies_df['Director'].isin(['Christopher Nolan', 'Ridley Scott'])].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `~` flips your booleans and allows you to find the inverse of your query.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df[~movies_df['Director'].isin(['Christopher Nolan', 'Ridley Scott'])].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a [Dataframe.query()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html) method that allows you to perform these conditional selections.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.query('Rating >= 8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.query('Director == \"Christopher Nolan\" | Director == \"Ridley Scott\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Question: \n",
    "\n",
    "Which group of movies has the higher average revenue those with a rating of above 8, or those with at least 300,000 reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset the dataframe to find movies with a rating above 8.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average revenue of that group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subset the dataframe to find movies with more than 300,000 reviews.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the average of that group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare the two numbers\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
